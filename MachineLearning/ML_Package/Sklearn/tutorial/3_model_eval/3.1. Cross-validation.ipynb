{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10bddc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec1f98",
   "metadata": {},
   "source": [
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test. Note that the word “experiment” is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally. Here is a flowchart of typical cross validation workflow in model training. The best parameters can be determined by grid search techniques.\n",
    "\n",
    "\n",
    "<img src='https://scikit-learn.org/stable/_images/grid_search_workflow.png' width=50% />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90b663c",
   "metadata": {},
   "source": [
    "In scikit-learn a random split into training and test sets can be quickly computed with the `train_test_split` helper function. Let’s load the iris data set to fit a linear support vector machine on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ecc86ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dccdc72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\t (90, 4) (90,)\n",
      "Test Data:\t (60, 4) (60,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "print('Train Data:\\t', X_train.shape, y_train.shape)\n",
    "\n",
    "print('Test Data:\\t', X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b320827c",
   "metadata": {},
   "source": [
    "When **evaluating different settings (“hyperparameters”)** for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can **“leak”** into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called **“validation set”**: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "\n",
    "However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n",
    "\n",
    "A solution to this problem is a procedure called **cross-validation** (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "* A model is trained using k-1 of the folds as training data;\n",
    "* the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "The performance measure reported by k-fold cross-validation is then **the average of the values computed in the loop**. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.\n",
    "\n",
    "<img src='https://scikit-learn.org/stable/_images/grid_search_cross_validation.png' width=60%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf801e39",
   "metadata": {},
   "source": [
    "# 3.1.1. Computing cross-validated metrics\n",
    "### Function:  `cross_val_score(clf, X, y, cv=cv)`\n",
    "The simplest way to use cross-validation is to call the `cross_val_score` helper function on the estimator and the dataset.\n",
    "\n",
    "The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d37ae5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96666667 1.         0.96666667 0.96666667 1.        ]\n",
      "0.98 accuracy with a standard deviation of 0.02\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(scores)\n",
    "# The mean score and the standard deviation are hence given by:\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8a24b3",
   "metadata": {},
   "source": [
    "By default, the score computed at each CV iteration is the `score` method of the estimator. It is possible to change this by using the scoring parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a5d6f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96658312, 1.        , 0.96658312, 0.96658312, 1.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring='f1_macro')\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7fbe86",
   "metadata": {},
   "source": [
    "Another option is to use an iterable yielding (train, test) splits as arrays of indices, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ff5fb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.97333333])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_cv_2folds(X):\n",
    "    n = X.shape[0]\n",
    "    i = 1\n",
    "    while i <= 2:\n",
    "        idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int)\n",
    "        yield idx, idx\n",
    "        i += 1\n",
    "\n",
    "custom_cv = custom_cv_2folds(X)\n",
    "cross_val_score(clf, X, y, cv=custom_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93573aa",
   "metadata": {},
   "source": [
    "## Data transformation with held out data\n",
    "### Function: `preprocessing.StandardScaler().fit(X_train)`\n",
    "\n",
    "Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization, feature selection, etc.) and similar data transformations similarly should be learnt from a training set and applied to held-out data for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b657f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score on transformed:0.9333333333333333, untransformed:0.35\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=0)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "clf = svm.SVC(C=1).fit(X_train_transformed, y_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "test_score_transformed = clf.score(X_test_transformed, y_test)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "print(f'test score on transformed:{test_score_transformed}, untransformed:{test_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda1a6b4",
   "metadata": {},
   "source": [
    "### Function: `make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))`\n",
    "A Pipeline makes it easier to compose estimators, providing this behavior under cross-validation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff2cbce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666667, 0.96666667, 0.96666667, 0.93333333, 1.        ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))\n",
    "cross_val_score(clf, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efa45fb",
   "metadata": {},
   "source": [
    "# 3.1.2. Cross validation iterators\n",
    "The following sections list utilities to generate indices that can be used to generate dataset splits according to different cross validation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f0a1d4",
   "metadata": {},
   "source": [
    "## 3.1.2.1. Cross-validation iterators for i.i.d. data\n",
    "\n",
    "### Function: `KFold(n_splits=n_splits)`\n",
    "KFold divides all the samples in k groups of samples, called folds (if k = n, this is equivalent to the Leave One Out strategy), of equal sizes (if possible). The prediction function is learned using k - 1 folds, and the fold left out is used for test.\n",
    "\n",
    "Example of 2-fold cross-validation on a dataset with 4 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad089bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train index: [2 3] \t test index: [0 1]\n",
      "train index: [0 1] \t test index: [2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = [\"a\", \"b\", \"c\", \"d\"]\n",
    "kf = KFold(n_splits=2)\n",
    "for train, test in kf.split(X):\n",
    "    print(f\"train index: {train} \\t test index: {test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653ad000",
   "metadata": {},
   "source": [
    "Here is a visualization of the cross-validation behavior. Note that KFold is not affected by classes or groups.\n",
    "<img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_006.png' width=70%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626b18a",
   "metadata": {},
   "source": [
    "### Function: `RepeatedKFold()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f97c96c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3] [0 1]\n",
      "[0 1] [2 3]\n",
      "[0 2] [1 3]\n",
      "[1 3] [0 2]\n",
      "[0 3] [1 2]\n",
      "[1 2] [0 3]\n",
      "[1 2] [0 3]\n",
      "[0 3] [1 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "random_state = 12883823\n",
    "rkf = RepeatedKFold(n_splits=2, n_repeats=4, random_state=random_state)\n",
    "for train, test in rkf.split(X):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3cf082",
   "metadata": {},
   "source": [
    "### Function: `ShuffleSplit()`\n",
    "The `ShuffleSplit` iterator will generate a user defined number of independent train / test dataset splits. Samples are first shuffled and then split into a pair of train and test sets.\n",
    "\n",
    "It is possible to control the randomness for reproducibility of the results by explicitly seeding the `random_state` pseudo random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b2ec5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 3 1 7 8 5] [2 9 6]\n",
      "[0 8 4 2 1 6 7] [9 5 3]\n",
      "[9 0 6 1 7 4 2] [8 3 5]\n",
      "[0 6 1 5 8 7 9] [2 4 3]\n",
      "[0 1 2 3 5 9 8] [6 4 7]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "X = np.arange(10)\n",
    "ss = ShuffleSplit(n_splits=5, test_size=0.25, random_state=1)\n",
    "for train_index, test_index in ss.split(X):\n",
    "    print(\"%s %s\" % (train_index, test_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38b88c6",
   "metadata": {},
   "source": [
    "Here is a visualization of the cross-validation behavior. Note that ShuffleSplit is not affected by classes or groups.\n",
    "ShuffleSplit is thus a good alternative to KFold cross validation that allows a finer control on the number of iterations and the proportion of samples on each side of the train / test split.\n",
    "<img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_008.png' width=70%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad7d9a",
   "metadata": {},
   "source": [
    "## 3.1.2.2. Cross-validation iterators with stratification based on class labels.\n",
    "\n",
    "### Function : `StratifiedKFold()`\n",
    "StratifiedKFold is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set. Here is an example of stratified 3-fold cross-validation on a dataset with 50 samples from two unbalanced classes. We show the number of samples in each class and compare with KFold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffbbf76",
   "metadata": {},
   "source": [
    "We can see that `StratifiedKFold` **preserves the class ratios** (approximately 1 / 10) in both train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f93858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stratified K-Fold! ===\n",
      "train [N neg, N Pos]:  [30  3]   |   test [N neg, N Pos]: [15  2]\n",
      "train [N neg, N Pos]:  [30  3]   |   test [N neg, N Pos]: [15  2]\n",
      "train [N neg, N Pos]:  [30  4]   |   test [N neg, N Pos]: [15  1]\n",
      "=== UnStratified K-Fold! ===\n",
      "train [N neg, N Pos]:   |   test [N neg, N Pos]:  [28  5]\n",
      "train [N neg, N Pos]:   |   test [N neg, N Pos]:  [28  5]\n",
      "train [N neg, N Pos]:   |   test [N neg, N Pos]:  [34]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import numpy as np\n",
    "X, y = np.ones((50, 1)), np.hstack(([0] * 45, [1] * 5))\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "print('=== Stratified K-Fold! ===')\n",
    "for train, test in skf.split(X, y):\n",
    "    print('train [N neg, N Pos]:  {}   |   test [N neg, N Pos]: {}'.format(np.bincount(y[train]), np.bincount(y[test])))\n",
    "\n",
    "\n",
    "print('=== UnStratified K-Fold! ===')\n",
    "kf = KFold(n_splits=3)\n",
    "for train, test in kf.split(X, y):\n",
    "    print('train [N neg, N Pos]:   |   test [N neg, N Pos]:  {}'.format(\n",
    "        np.bincount(y[train]), np.bincount(y[test])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc97624",
   "metadata": {},
   "source": [
    "### Function: `StratifiedShuffleSplit()`\n",
    "StratifiedShuffleSplit is a variation of `ShuffleSplit`, which returns stratified splits, i.e which creates splits by preserving the same percentage for each target class as in the complete set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ee1f0",
   "metadata": {},
   "source": [
    "## 3.1.2.3. Cross-validation iterators for grouped data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6de9f4",
   "metadata": {},
   "source": [
    "**The i.i.d. assumption is broken if the underlying generative process yield groups of dependent samples.**\n",
    "\n",
    "Such a grouping of data is domain specific. An example would be when there is medical data collected from multiple patients, with multiple samples taken from each patient. And such data is likely to be dependent on the individual group. In our example, the patient id for each sample will be its group identifier.\n",
    "\n",
    "In this case we would like to know if a model trained on a particular set of groups generalizes well to the unseen groups. To measure this, we need to ensure that all the samples in the validation fold come from groups that are not represented at all in the paired training fold.\n",
    "\n",
    "The following cross-validation splitters can be used to do that. The grouping identifier for the samples is specified via the groups parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a13b274",
   "metadata": {},
   "source": [
    "### Function : `GroupKFold()`\n",
    "GroupKFold is a variation of k-fold which ensures that the same group is not represented in both testing and training sets. For example if the data is obtained from different subjects with several samples per-subject and if the model is flexible enough to learn from highly person specific features it could fail to generalize to new subjects. GroupKFold makes it possible to detect this kind of overfitting situations.\n",
    "\n",
    "Imagine you have three subjects, each with an associated number from 1 to 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6fde7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5] [6 7 8 9]\n",
      "[0 1 2 6 7 8 9] [3 4 5]\n",
      "[3 4 5 6 7 8 9] [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]\n",
    "y = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"d\", \"d\", \"d\"]\n",
    "groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n",
    "\n",
    "gkf = GroupKFold(n_splits=3)\n",
    "for train, test in gkf.split(X, y, groups=groups):\n",
    "    print(\"%s %s\" % (train, test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3e68ac",
   "metadata": {},
   "source": [
    "Each subject is in a different testing fold, and the same subject is never in both testing and training. Notice that the folds do not have exactly the same size due to the imbalance in the data.\n",
    "\n",
    "Here is a visualization of the cross-validation behavior.\n",
    "\n",
    "<img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_007.png' width=50%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52451d3",
   "metadata": {},
   "source": [
    "### Function: `StratifiedGroupKFold()`\n",
    "StratifiedGroupKFold is a cross-validation scheme that combines both StratifiedKFold and GroupKFold. The idea is to try to preserve the distribution of classes in each split while keeping each group within a single split. That might be useful when you have an unbalanced dataset so that using just GroupKFold might produce skewed splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ccecd508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  6  7 10 11 15 16 17] [ 1  8  9 12 13 14]\n",
      "[ 0  1  4  5  6  7  8  9 11 12 13 14] [ 2  3 10 15 16 17]\n",
      "[ 1  2  3  8  9 10 12 13 14 15 16 17] [ 0  4  5  6  7 11]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "X = list(range(18))\n",
    "y = [1] * 6 + [0] * 12\n",
    "groups = [1, 2, 3, 3, 4, 4, 1, 1, 2, 2, 3, 4, 5, 5, 5, 6, 6, 6]\n",
    "sgkf = StratifiedGroupKFold(n_splits=3)\n",
    "for train, test in sgkf.split(X, y, groups=groups):\n",
    "    print(\"%s %s\" % (train, test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885fae48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
